{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c5afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"log.txt\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518281cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'/home/mohammad/Tokenizers/tokenizers/custom_bert_tokenizer')\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fb6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dfa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_col.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234e4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ed541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [str(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195eebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'text':text_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70d041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc18a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples['text'], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e42e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d44d7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff51470",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(\n",
    "                        test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b002295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ba966f45324b4cabd3a7131b8bb2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/3724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6ed00c97a3442e8586b0c58c3ed889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89cc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80df294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 05:04:27,855 [INFO] Created a temporary directory at /tmp/tmpv62grc_2\n",
      "2023-08-06 05:04:27,856 [INFO] Writing /tmp/tmpv62grc_2/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc58705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c214c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30522\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f9e403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e069569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db33a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77186d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1f01327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",  # to evaluate model and get metrics after each epoch\n",
    "    logging_strategy=\"epoch\",  # to log metrics after each epoch\n",
    "    save_strategy=\"epoch\",  # to save model after each epoch\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-2,\n",
    "    num_train_epochs=3,   \n",
    "    logging_dir='./logs', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df097c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 3724\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 931\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f42fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbf35604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3,724\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Training with DataParallel so batch size has been adjusted to: 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 699\n",
      "  Number of trainable parameters = 109,514,298\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='699' max='699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [699/699 09:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.233100</td>\n",
       "      <td>7.051541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.021000</td>\n",
       "      <td>7.004931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.978000</td>\n",
       "      <td>6.982363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/checkpoint-233\n",
      "Configuration saved in test_trainer/checkpoint-233/config.json\n",
      "Configuration saved in test_trainer/checkpoint-233/generation_config.json\n",
      "Model weights saved in test_trainer/checkpoint-233/pytorch_model.bin\n",
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/checkpoint-466\n",
      "Configuration saved in test_trainer/checkpoint-466/config.json\n",
      "Configuration saved in test_trainer/checkpoint-466/generation_config.json\n",
      "Model weights saved in test_trainer/checkpoint-466/pytorch_model.bin\n",
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/checkpoint-699\n",
      "Configuration saved in test_trainer/checkpoint-699/config.json\n",
      "Configuration saved in test_trainer/checkpoint-699/generation_config.json\n",
      "Model weights saved in test_trainer/checkpoint-699/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=699, training_loss=7.410704082003308, metrics={'train_runtime': 552.3585, 'train_samples_per_second': 20.226, 'train_steps_per_second': 1.265, 'total_flos': 2940524168601600.0, 'train_loss': 7.410704082003308, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a89cd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history=trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "395eed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_losses = []\n",
    "train_losses = []\n",
    "train_time = 0.0\n",
    "epochs = []\n",
    "lr = []\n",
    "for history_dict in training_history:\n",
    "    \n",
    "    try:\n",
    "        if 'eval_loss' in history_dict.keys():\n",
    "            valid_loss = history_dict['eval_loss']\n",
    "            valid_losses.append(valid_loss)\n",
    "        elif 'loss' in history_dict.keys():\n",
    "            train_loss = history_dict['loss']\n",
    "            epochs.append(history_dict['epoch'])\n",
    "            train_losses.append(train_loss)\n",
    "            lr.append(history_dict['learning_rate'])\n",
    "        elif 'train_runtime' in history_dict.keys():\n",
    "            train_time = history_dict['train_runtime']\n",
    "    except Exception as e:\n",
    "        print(f'Something error {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf65fe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.051540851593018, 7.0049309730529785, 6.982363224029541],\n",
       " [8.2331, 7.021, 6.978],\n",
       " 552.3585,\n",
       " [1.0, 2.0, 3.0],\n",
       " [0.013333333333333332, 0.006666666666666666, 0.0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_losses,train_losses,train_time,epochs,lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d485336",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times = [train_time/len(valid_losses)]*len(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "345ee0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'epochs':epochs,'train_losses':train_losses,'valid_losses':valid_losses,'train_times':train_times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca399de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4921c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.to_csv('logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2710f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\toptimizer.pt\t   rng_state.pth  trainer_state.json\r\n",
      "generation_config.json\tpytorch_model.bin  scheduler.pt   training_args.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18046955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,BertForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'/home/mohammad/Tokenizers/tokenizers/custom_bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f774581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BertForMaskedLM.from_pretrained('../ali_bert/checkpoint-932')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bda1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"club nacional de [MASK] miho conoci como nacional ta club mas grandi di futbol di montevidéu uruguay fundá dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de león uruguay nicolás lodeiro uruguay héctor scarone uruguay julio cesar dely valdéz panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f279ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc4da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=pipeline('fill-mask', model=model1, tokenizer=tokenizer,device=model1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc18b25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07844258844852448,\n",
       "  'token': 602,\n",
       "  'token_str': 'di',\n",
       "  'sequence': 'club nacional de di miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.046052489429712296,\n",
       "  'token': 607,\n",
       "  'token_str': 'ku',\n",
       "  'sequence': 'club nacional de ku miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.03891874477267265,\n",
       "  'token': 606,\n",
       "  'token_str': 'ta',\n",
       "  'sequence': 'club nacional de ta miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.0230559092015028,\n",
       "  'token': 610,\n",
       "  'token_str': 'pa',\n",
       "  'sequence': 'club nacional de pa miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.02198743261396885,\n",
       "  'token': 620,\n",
       "  'token_str': 'un',\n",
       "  'sequence': 'club nacional de un miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6adeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali_nlp",
   "language": "python",
   "name": "ali_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
