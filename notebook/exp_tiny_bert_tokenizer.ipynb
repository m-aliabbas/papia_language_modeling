{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9a65eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m-aliabbas1\r\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf2db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "user_id = HfApi().whoami()[\"name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a1a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0941e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id=\"bert-tiny-waspak-2023-papia1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4883518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ali.txt\t\t\t exp_notebooks\t\t\t    models\r\n",
      "clean_filenames.csv\t miniconda3\t\t\t    text_col.csv\r\n",
      "exp_language_model_work  Miniconda3-latest-Linux-x86_64.sh  tokenizer_papia\r\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd62896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../clean_filenames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ea4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.columns[:2],axis=1)\n",
    "\n",
    "df = df.rename(columns={'data':'text'})\n",
    "\n",
    "df.to_csv('text_col.csv',index=False)\n",
    "\n",
    "text_list = list(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08a623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=100):\n",
    "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "        batch = text_list[i : i + batch_size]\n",
    "        yield [str(text) for text in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f90cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a74031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc82f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option max_length\n",
      "Ignored unknown kwargs option model_max_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:10<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30522\n",
    "\n",
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=vocab_size,max_length=512,model_max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "270f0b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_tiny_bert_papia/tokenizer_config.json',\n",
       " 'tokenizer_tiny_bert_papia/special_tokens_map.json',\n",
       " 'tokenizer_tiny_bert_papia/vocab.txt',\n",
       " 'tokenizer_tiny_bert_papia/added_tokens.json',\n",
       " 'tokenizer_tiny_bert_papia/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.save_pretrained(\"tokenizer_tiny_bert_papia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1bda4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/m-aliabbas1/bert-tiny-waspak-2023-papia1/commit/48c2aeb4fc0a22cd6dc223cf0a396711d4a59585', commit_message='Upload tokenizer', commit_description='', oid='48c2aeb4fc0a22cd6dc223cf0a396711d4a59585', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.push_to_hub(tokenizer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effa665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d33e692fe34112981e68ce6dbea3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10522a1e96640a881917e5c8826b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/708k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{user_id}/{tokenizer_id}')\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cdfc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [str(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd731c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'text':text_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1befcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "980c1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "#     print(examples['text'])\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples['text'], return_special_tokens_mask=True, truncation=True, max_length=512\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc92c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abca562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce43eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(\n",
    "    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b8bee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9564bb3b42a644d498f5ed13bc7722cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/3724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65258355684c4dfdb602975006bdf0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f425ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "019cc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf4191c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a8b0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30522\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b43c519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deedb6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mohammad/.cache/huggingface/hub/models--prajjwal1--bert-tiny/snapshots/6f75de8b60a9f8a2fdf7b69cbd86d9e64bcb3837/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained('prajjwal1/bert-tiny',max_position_embeddings=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b7a76a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7346e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "334a8654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17bff6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a317aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4eff41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 30\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=40, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=30,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=30,\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c115291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c258752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a782bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3,724\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 40\n",
      "  Training with DataParallel so batch size has been adjusted to: 80\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 640\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 4,416,698\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 07:17, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.686400</td>\n",
       "      <td>8.629770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to models/checkpoint-30\n",
      "Configuration saved in models/checkpoint-30/config.json\n",
      "Configuration saved in models/checkpoint-30/generation_config.json\n",
      "Model weights saved in models/checkpoint-30/pytorch_model.bin\n",
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/checkpoint-30 (score: 8.629770278930664).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=8.652171325683593, metrics={'train_runtime': 446.9944, 'train_samples_per_second': 83.312, 'train_steps_per_second': 0.112, 'total_flos': 43262920949760.0, 'train_loss': 8.652171325683593, 'epoch': 8.51})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66192d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/checkpoint-30/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/checkpoint-30/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at models/checkpoint-30.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file models/checkpoint-30/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db427340",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer,device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a02c0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'club nacional de football miho conoci como nacional ta club mas grandi di futbol di montevidéu uruguay fundá dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de león uruguay nicolás lodeiro uruguay héctor scarone uruguay julio cesar dely valdéz panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920 1922 1923 1924 1933 1934 1939 1940 1941 1942 1943 1946 1947 1950 1952 1955 1956 1957 1963 1966 1969 1970 1971 1972 1977 1980 1983 1992 1998 2000 2001 2002 2005 2005 06 2008 09 2010 11 2011 12 2014 15 2016 campeon kontinental sur amerika cup 3 1971 nacional 2 0 estudiantes di argentina 1980 nacional 1 0 internacional di brazil 1988 nacional 3 0 newell old boys di argentina recopa sur amerika cup 1 1989 nacional 1 0 racing club di argentina campeon inter kontinental mundu cup di club 3 1971 nacional 2 1 panathinaikos di hellas 1981 nacional 1 0 nottigham forest di inglatera 1988 nacional 2 2 7 6 psv eindovhen di hulanda copa inter amerika 2 1972 nacional 2 1 cruz azul di mexico 1989 nacional 4 0 olimpia di honduras link externo pagina oficial pagina fan pagina fan'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41dfec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"club nacional de [MASK] miho conoci como nacional ta club mas grandi di futbol di montevidéu uruguay fundá dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de león uruguay nicolás lodeiro uruguay héctor scarone uruguay julio cesar dely valdéz panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5830825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0007799027371220291,\n",
       "  'token': 602,\n",
       "  'token_str': 'di',\n",
       "  'sequence': 'club nacional de di miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.0005202289903536439,\n",
       "  'token': 606,\n",
       "  'token_str': 'ta',\n",
       "  'sequence': 'club nacional de ta miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.0004961944650858641,\n",
       "  'token': 625,\n",
       "  'token_str': 'na',\n",
       "  'sequence': 'club nacional de na miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.00048412513569928706,\n",
       "  'token': 996,\n",
       "  'token_str': 'nobo',\n",
       "  'sequence': 'club nacional de nobo miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'},\n",
       " {'score': 0.00043325821752659976,\n",
       "  'token': 607,\n",
       "  'token_str': 'ku',\n",
       "  'sequence': 'club nacional de ku miho conoci como nacional ta club mas grandi di futbol di montevideu uruguay funda dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de leon uruguay nicolas lodeiro uruguay hector scarone uruguay julio cesar dely valdez panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2dab4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpa26nfhei/config.json\n",
      "Configuration saved in /tmp/tmpa26nfhei/generation_config.json\n",
      "Model weights saved in /tmp/tmpa26nfhei/pytorch_model.bin\n",
      "Uploading the following files to m-aliabbas1/bert-tiny-waspak-2023-papia1: generation_config.json,config.json,pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7704c872f74b1fa1ba30f959c1c320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/m-aliabbas1/bert-tiny-waspak-2023-papia1/commit/d868cc937188f111548924dc3473d886446f1da9', commit_message='Upload BertForMaskedLM', commit_description='', oid='d868cc937188f111548924dc3473d886446f1da9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('bert-tiny-waspak-2023-papia1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c35ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali_nlp",
   "language": "python",
   "name": "ali_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
