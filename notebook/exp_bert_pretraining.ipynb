{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f64838",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id=\"bert-base-waspak-2023-papia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c986d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m-aliabbas1\r\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee56133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "user_id = HfApi().whoami()[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5afd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518281cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{user_id}/{tokenizer_id}')\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fb6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07dfa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_col.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234e4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ed541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [str(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195eebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'text':text_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70d041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc18a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples['text'], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e42e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d44d7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b002295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e100acb564f943d08b1ba6317df32232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/4655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89cc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80df294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc58705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/envs/ali_nlp/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c214c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30522\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9e403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e069569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db33a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77186d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25e986df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "728778cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n",
      "Current device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available and print status\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device and print it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98478bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4,655\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Training with DataParallel so batch size has been adjusted to: 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 160\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 290\n",
      "  Number of trainable parameters = 109,514,298\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 30:08, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=290, training_loss=6.778239493534483, metrics={'train_runtime': 1814.8243, 'train_samples_per_second': 25.65, 'train_steps_per_second': 0.16, 'total_flos': 1.3243150156032e+16, 'train_loss': 6.778239493534483, 'epoch': 9.96})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdbe2ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'club nacional de football miho conoci como nacional ta club mas grandi di futbol di montevidéu uruguay fundá dia 14 di mei 1899 club ta resultado di union entre uruguay athletic montevideo football club uruguay athletic tabata un club di bario la union cual no mester wordo confundi cu uruguay athletic club cu tabata hunga den prome division actualmente nacional ta hunga den liga profesional mas halto na uruguay algun futbolista ku tabata hunga pa nacional ta luis suarez uruguay sebastian abreu uruguay atilio garcia argentina hugo de león uruguay nicolás lodeiro uruguay héctor scarone uruguay julio cesar dely valdéz panama fernando muslera uruguay titulos campeon nashonal liga profesional di uruguay 45 1902 1903 1912 1915 1916 1917 1919 1920 1922 1923 1924 1933 1934 1939 1940 1941 1942 1943 1946 1947 1950 1952 1955 1956 1957 1963 1966 1969 1970 1971 1972 1977 1980 1983 1992 1998 2000 2001 2002 2005 2005 06 2008 09 2010 11 2011 12 2014 15 2016 campeon kontinental sur amerika cup 3 1971 nacional 2 0 estudiantes di argentina 1980 nacional 1 0 internacional di brazil 1988 nacional 3 0 newell old boys di argentina recopa sur amerika cup 1 1989 nacional 1 0 racing club di argentina campeon inter kontinental mundu cup di club 3 1971 nacional 2 1 panathinaikos di hellas 1981 nacional 1 0 nottigham forest di inglatera 1988 nacional 2 2 7 6 psv eindovhen di hulanda copa inter amerika 2 1972 nacional 2 1 cruz azul di mexico 1989 nacional 4 0 olimpia di honduras link externo pagina oficial pagina fan pagina fan'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab28d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or simply use pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer,device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d7287fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.12758685648441315, 'token': 602, 'token_str': 'di', 'sequence': 'club nacional de football miho di como nnda dia 14 dy'}\n",
      "{'score': 0.04371142014861107, 'token': 606, 'token_str': 'ta', 'sequence': 'club nacional de football miho ta como nnda dia 14 dy'}\n",
      "{'score': 0.03310277685523033, 'token': 620, 'token_str': 'un', 'sequence': 'club nacional de football miho un como nnda dia 14 dy'}\n",
      "{'score': 0.018927378579974174, 'token': 628, 'token_str': 'den', 'sequence': 'club nacional de football miho den como nnda dia 14 dy'}\n",
      "{'score': 0.01463568489998579, 'token': 625, 'token_str': 'na', 'sequence': 'club nacional de football miho na como nnda dia 14 dy'}\n"
     ]
    }
   ],
   "source": [
    "example = \"'club nacional de football miho [MASK] como nndá dia 14 dy\"\n",
    "for prediction in fill_mask(example):\n",
    "  print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1da107ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 24 17:05:18 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3090         Off| 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   45C    P8               25W / 370W|  20745MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce RTX 3090         Off| 00000000:08:00.0 Off |                  N/A |\r\n",
      "|  0%   41C    P8               26W / 370W|  17741MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A     56626      C   ...miniconda3/envs/ali_nlp/bin/python3    20742MiB |\r\n",
      "|    1   N/A  N/A     56626      C   ...miniconda3/envs/ali_nlp/bin/python3    17738MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c149b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali_nlp",
   "language": "python",
   "name": "ali_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
